## Machine Learning

机器学习定义：

![image-20201109214609524](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201109214609524.png)

主要学习算法：监督学习和无监督学习

##### 监督学习：

分类：回归、分类

通过已知类别的数据来获取到一个函数，再通过这个函数再判断其他数据（利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程

##### 无监督学习：

通过不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系

分类：聚类算法

## 单变量线性回归

##### 模式表示：

回归：通过线性回归算法和数据来建立一个模型预测房价

##### 代价函数：

模型所预测的值与训练集中实际值之间的差距就是建模误差

代价函数(平方误差函数或平方误差代价函数)：可能是解决回归问题最常用的手段

建模选择合适的参数![image-20201114214348307](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201114214348307.png)模式所预测的值与训练集中实际值之间的差距就是建模误差，选择出使得建模误差的平方和能够最小的模型参数，即使得代价函数![image-20201114214640480](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201114214640480.png)最小。(𝑥 代表特征/输入变量  ;𝑦 代表目标变量/输出变量  ;(𝑥(𝑖), 𝑦(𝑖)) 代表第𝑖 个观察实例  )

##### 梯度下降：

梯度下降是一个用来求函数最小值的算法，我们可以通过使用梯度下降算法来求出上节代价函数的最小值

![image-20201112145913645](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201112145913645.png)

该算法为称为批量梯度下降算法，指的是梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所有，在每一个单独的梯度下降中，我们都要计算，这个项需要对所有m个训练样本求和。



## 线代代数回顾

矩阵和向量的乘法:

m x n的矩阵乘以n x 1的向量，得到的是m x 1的向量

矩阵乘法

矩阵的逆：如矩阵𝐴是一个𝑚 × 𝑚矩阵（方阵），如果有逆矩阵，则：![image-20201112164417333](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201112164417333.png)

矩阵的转置：设𝐴为𝑚 × 𝑛阶矩阵（即𝑚行𝑛列），第𝑖行𝑗列的元素是𝑎(𝑖, 𝑗)，即： 𝐴 = 𝑎(𝑖, 𝑗)
定义𝐴的转置为这样一个𝑛 × 𝑚阶矩阵𝐵，满足𝐵 = 𝑎(𝑗, 𝑖)，即 𝑏(𝑖, 𝑗) = 𝑎(𝑗, 𝑖)（ 𝐵的第𝑖行
第𝑗列元素是𝐴的第𝑗行第𝑖列元素），![image-20201112164434701](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201112164434701.png) 



## 多变量线性回归

比如上节房价模型增加更多的特征，如房间数量、楼层等，构成一个含有多个变量的模型，通过回归和数据建立一个多变量线性回归模型。

在多变量线性回归中，我们也构建一个代价函数：

代价函数是所有建模误差的平方和，即：

![image-20201113211218759](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201113211218759.png)

其中：![image-20201113211234725](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201113211234725.png)

要找出代价函数最小的一系列参数，多变量线性回归的批量梯度下降算法：

![image-20201113211553361](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201113211553361.png)

让梯度下降算法更快地收敛：

1. 特征缩放
2. 学习率

如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要

##### 正规方程：

训练集特征矩阵为 𝑋（包含了 ![image-20201114212739117](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201114212739117.png)）并且我们的训练集结果为向量 𝑦，  利用正规方程解出向量

​		![image-20201114212044946](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201114212044946.png)

可以解决线性回归问题

![image-20201113223243720](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201113223243720.png)

## 逻辑回归

##### 分类问题

在分类问题中，你要预测的变量y是离散的值，我们将学习一种叫做逻辑回归的算法.在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）

###### 二元的分类问题：

因变量可能属于的两个类分别称为负向类和正向类。其中0代表负向类，1代表正向类

逻辑回归算法的性质是它的输出值永远在0与1之间。它是一个分类算法

##### 假说表示

逻辑回归的模型：该模型的输出变量范围始终在0与1之间。

逻辑回归模型的假设是：

![image-20201117211350555](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201117211350555.png)

g代表逻辑函数，是一个常用的逻辑函数为S形函数，公式为:![image-20201117211625670](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201117211625670.png)

该函数的图像为：

![image-20201117212348916](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201117212348916.png)

![image-20201117212404999](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201117212404999.png)

##### 判定边界

决策边界不是训练集的属性，而是假设本身及其参数的属性，只要给定了参数向量𝜃  就能确定。

##### 代价函数

###### 如何拟合逻辑回归模型的参数𝜃  

用来拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题。

![image-20201117213727649](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201117213727649.png)



逻辑回归的代价函数:![image-20201117214248455](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201117214248455.png)

![image-20201117214408545](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201117214408545.png)

##### 简化代价函数与梯度下降

找出一种稍微简单一点的方法来写代价函数，来替换我们现在用的方法。同时我们还要弄清楚如何运用梯度下降法，来拟合出逻辑回归的参数。  

𝐶𝑜𝑠𝑡(ℎ𝜃(𝑥), 𝑦) = -𝑦 × 𝑙𝑜𝑔(ℎ𝜃(𝑥)) - (1 - 𝑦) × 𝑙𝑜𝑔(1 - ℎ𝜃(𝑥))

###### 逻辑回归的代价函数：

![image-20201118225630760](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201118225630760.png)

##### 高级优化

利用高级优化方法，我们就能够使通过梯度下降，进行逻辑回归的速度大大提高，而这也将使算法更加适合解决大型的机
器学习问题  。高级优化方法比如共轭梯度法 BFGS (变尺度法) 和 L-BFGS (限制变尺度法)  

##### 多类别分类：一对多

使用逻辑回归 (logistic regression)来解决多类别分类问题  

## 正则化

##### 过拟合的问题  

过拟合是过于强调拟合原始数据，而丢失了算法的本质：预测新数据  。(及对给出一个新的值去预测，将表现很差)

如 下图回归问题，右图为过度拟合，左图刚好合适：

![image-20201119164544829](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201119164544829.png)

正则化(regularization)的技术可以改善或者减少过度拟合问题。  

解决过拟合化的方法：

1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA）  
2. 正则化。 保留所有的特征，但是减少参数的大小（ magnitude）。  

##### 代价函数

上面的回归问题中如果我们的模型是：
![image-20201119171325758](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201119171325758.png)
我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于 0 的话，我们就能很好的拟合了。  

所以我们要做的就是在一定程度上减小这些参数𝜃 的值，这就是正则化的基本方法。  

![image-20201120154243642](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120154243642.png)
其中𝜆又称为正则化参数（ Regularization Parameter）。  

##### 正则化线性回归

正则化线性回归的代价函数为：![image-20201120154408181](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120154408181.png)

梯度下降算法：

![image-20201120154512073](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120154512073.png)

正规方程解决正则化线性回归模型：

![image-20201120154527645](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120154527645.png)

##### 正则化的逻辑回归模型：

逻辑回归模型的代价函数：

![image-20201120154629832](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120154629832.png)

梯度下降算法为：

![image-20201120154655600](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120154655600.png)

![image-20201120154708828](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120154708828.png)

## 神经网络

##### 非线性假设

无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。  这时候我们需要神经网络。

##### 神经元和大脑

神经网络是一种很古老的算法 ，它最初产生的目的是制造能模拟大脑的机器。  

每一个神经元都可以被认为是一个处理单元/神经核（ processing unit/Nucleus），它含有许多输入/树突（ input/Dendrite），并且有一个输出/轴突（ output/Axon）。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。

##### 模式表示

下面以逻辑回归模型作为自身学习模型的神经元示例。

![image-20201120203130077](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120203130077.png)

其中𝑥1, 𝑥2, 𝑥3是输入单元（ input units），我们将原始数据输入给它们。𝑎1, 𝑎2, 𝑎3是中间单元，它们负责将数据进行处理，然后呈递到下一层。最后是输出单元，它负责计算ℎ𝜃(𝑥)  。

下图为一个 3 层的神经网络，第一层成为输入层（ Input Layer），最后一层称为输出层（ Output Layer），中间一层成为隐藏层（ Hidden Layers）。我们为每一层都增加一个偏差单位（ bias unit）：  

![image-20201120203259540](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120203259540.png)

![image-20201120203420733](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120203420733.png)

上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。我们可以知道：每一个𝑎都是由上一层所有的𝑥和每一个𝑥所对应的决定的。（我们把这样从左到右的算法称为**前向传播算法**( **FORWARD PROPAGATION** )）  

神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。  

![image-20201120221808940](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201120221808940.png)

##### 多类分类

当我们有不止两种分类时（也就是𝑦 = 1,2,3 ….），比如以下这种情况，该怎么办？如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有 4 个值。例如，第一个值为 1 或 0 用于预测是否是行人，第二个值用于判断是否为汽车。
输入向量𝑥有三个维度，两个中间层，输出层 4 个神经元分别用来表示 4 类，也就是每一个数据在输出层都会出现[𝑎 𝑏 𝑐 𝑑]𝑇，且𝑎, 𝑏, 𝑐, 𝑑中仅有一个为 1，表示当前类。  

##### 代价函数

![image-20201122165155314](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122165155314.png)

我们的ℎ𝜃(𝑥)是一个维度为𝐾的向量  

##### 反向传播算法

需要使得代价函数最小

###### 向前传播算法

![image-20201122170505963](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122170505963.png)

为了计算导数项，我们将采用一种反向传播的算法

误差是激活单元的预测与实际值之间的误差。（k = 1:k)

​	![image-20201122190404887](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122190404887.png)

我们利用这个误差值来计算前一层的误差：

​		![image-20201122191121945](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122191121945.png)

其中:![image-20201122191339604](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122191339604.png)

第二层误差：![image-20201122191659819](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122191659819.png)

因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设𝜆 = 0，即我们不做任何正则化处理时有：  

![image-20201122192537287](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122192537287.png)

- 𝑙 代表目前所计算的是第几层。
- 𝑗 代表目前计算层中的激活单元的下标，也将是下一层的第𝑗个输入变量的下标。
- 𝑖 代表下一层中误差单元的下标，是受到权重矩阵中第𝑖行影响的下一层中的误差单元的下标  

如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误
差单元也是一个矩阵，我们用![image-20201122201007065](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122201007065.png)来表示这个误差矩阵。第 𝑙 层的第 𝑖 个激活单元受到第 𝑗个参数影响而导致的误差  

![image-20201122201150518](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122201150518.png)

##### 梯度检验

当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。为了避免这样的问题，我们采取一种叫做梯度的数值检验（ Numerical Gradient Checking）
方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 𝜃，我们计算出在 𝜃-𝜀 处和 𝜃+𝜀 的代价值（ 𝜀是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 𝜃处的代价值。  

![image-20201122211013984](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201122211013984.png)

##### 随机初始化

任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我
们初始所有的参数都为一个非 0 的数，结果也是一样的。  

为了防止出现上述情况，在参数进行初始化时进行随机初始化

##### 总体回顾神经网络

## 应用机器学习的建议

#### 机器学习诊断方法

定义：是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够
告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。  

##### 怎么评价你的算法

为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用 70%的数据作为训练集，用剩下 30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据， 通常我们要对数据进行“洗牌”， 然后再分成训练集和测试集。  

##### 模型选择问题

![image-20201201172447120](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201201172447120.png)

显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 即 ：使用 60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用 20%的数据作为测试集。

**模型选择方法：**

![image-20201201173026439](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201201173026439.png)

1. 使用训练集训练出10个训练集
2. 使用10个模型分别对交叉验证集计算出交叉验证误差（代价函数的值）
3. 选取代价函数值最小的模型
4. 用3中选出的模型对测试集计算得出推广误差(代价函数的值)

##### 诊断偏差与方差

当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？
搞清楚这一点非常重要，因为能判断出现的情况是这两种情况中的哪一种。其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。  

通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析:  

  ![image-20201201200430846](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201201200430846.png)

对于训练集，当 𝑑 较小时，模型拟合程度更低，误差较大；随着 𝑑 的增长，拟合程度提高，误差减小。

对于交叉验证集，当 𝑑 较小时，模型拟合程度低，误差较大；但是随着 𝑑 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。

**训练集误差和交叉验证集误差近似时：偏差/欠拟合**
**交叉验证集误差远大于训练集误差时： 方差/过拟合**   

##### 正则化和偏差/方差

在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了， 即我们在选择 λ 的值时也需要思考与刚才选择多项式模型次数类似的问题。

**选择𝜆的方法为：**

1. 使用训练集训练出 12 个不同程度正则化的模型  
2. 用 12 个模型分别对交叉验证集计算的出交叉验证误  
3. 选择得出交叉验证误差最小的模型  
4. 运用步骤 3 中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与 λ 的值绘制在一张图表。  

![image-20201201204641321](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201201204641321.png)

- 当 𝜆 较小时，训练集误差较小（过拟合）而交叉验证集误差较大  
- 随着 𝜆 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加  

##### 学习曲线

学习曲线是来判断某一个学习算法是否处于偏差、方差问题，学习曲线是学习算法的一个很好的合理检验（ sanity check）。

如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观，也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。  

如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果，也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。  

![image-20201201222124087](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201201222124087.png)

## 机器学习系统的设计

##### 首先要做什么

##### 误差分析

构建一个学习算法的推荐方法为：

1. 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法
2. 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择  
3. 进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势 

##### 偏斜类的误差度量

设定某个实数来评估你的学习算法，并衡量它的表现，有了算法的评估和误差度量值。有一件重要的事情要注意，就是使用一个合适的误差度量值，这有时会对于你的学习算法造成非常微妙的影响，这件重要的事情就是偏斜类（ skewed classes）的问题。  

偏斜类简单理解就是在训练模型时由于正样本和负样本之间的严重不平衡，导致模型最后检测全部都是1或者全部都是0。偏斜类情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。

例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有 0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有 0.5%。然而我们通过训练而得到的神经网络算法却有 1%的误差。这时，误差的
大小是不能视为评判算法效果的依据的。

![image-20201203163603736](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201203163603736.png)

#####   查准率和查全率之间的权衡  

假使，我们的算法输出的结果在 0-1 之间，我们使用阀值 0.5 来预测真和假  

![image-20201203170720969](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201203170720969.png)

我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表， 曲线的形状根据数据的不同而不同。

一个帮助我们选择这个阀值的方法。一种方法是计算 F1 值（ F1 Score），其计算公式为：

![image-20201203170827992](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201203170827992.png)

  我们选择使得 F1 值最高的阀值 

##### 机器学习的数据

## 支持向量机(Support Vector Machines)

支持向量机，或者简称 SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。  

##### 优化目标

我们开始学习这个算法。为了描述支持向量机，事实上，我将会从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量机。  

![image-20201204191653138](C:\Users\Ally\AppData\Roaming\Typora\typora-user-images\image-20201204191653138.png)



  

